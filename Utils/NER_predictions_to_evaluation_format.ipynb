{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convert predicted entities in the evaluation format"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define the imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define paths to the prediction files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH_NER_PREDICTIONS = \"../Predictions/NER/gliner_biomed_finetuned3_predicted_entities.json\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define output path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH_OUTPUT_NER_PREDICTIONS = \"../Predictions/NER/gliner_biomed_finetuned3_predicted_entities_eval_format.json\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the input files into dictionary variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(PATH_NER_PREDICTIONS, 'r', encoding='utf-8') as file:\n",
    "\tner_predictions = json.load(file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define the functions to process NER predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Merge consecutive NER predictions "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_consecutive_predictions(data):\n",
    "    \"\"\"\n",
    "    Parse and merge consecutive predicted entities in the input dictionary where start and end indices are sequential.\n",
    "    \"\"\"\n",
    "    print(f\"Merging consecutive NER predictions...\")\n",
    "\n",
    "    # Process each document\n",
    "    for pmid, doc in data.items():\n",
    "        merged_entities = []\n",
    "        current_entity = None\n",
    "\n",
    "        for entity in doc.get(\"pred_entities\", []):\n",
    "            if current_entity is None:\n",
    "                # Start a new entity\n",
    "                current_entity = entity\n",
    "            else:\n",
    "                # Check if the current entity should be merged with the previous one\n",
    "                if (\n",
    "                    current_entity[\"end_idx\"] + 1 == entity[\"start_idx\"] and\n",
    "                    current_entity[\"entity_label\"] == entity[\"entity_label\"]\n",
    "                ):\n",
    "                    # Merge entities by extending the current entity\n",
    "                    current_entity[\"end_idx\"] = entity[\"end_idx\"]\n",
    "                    current_entity[\"text_span\"] += \" \" + entity[\"text_span\"]\n",
    "                    current_entity[\"score\"] = min(current_entity[\"score\"], entity[\"score\"])\n",
    "                elif(\n",
    "                    current_entity[\"end_idx\"] == entity[\"start_idx\"] and\n",
    "                    current_entity[\"entity_label\"] == entity[\"entity_label\"]\n",
    "                ):\n",
    "                    # Merge entities by extending the current entity\n",
    "                    current_entity[\"end_idx\"] = entity[\"end_idx\"]\n",
    "                    current_entity[\"text_span\"] += entity[\"text_span\"]\n",
    "                    current_entity[\"score\"] = min(current_entity[\"score\"], entity[\"score\"])\n",
    "                else:\n",
    "                    # Append the completed entity and start a new one\n",
    "                    merged_entities.append(current_entity)\n",
    "                    current_entity = entity\n",
    "\n",
    "        # Append the last entity if any\n",
    "        if current_entity is not None:\n",
    "            merged_entities.append(current_entity)\n",
    "\n",
    "        # Replace the original entities with the merged ones\n",
    "        doc[\"pred_entities\"] = merged_entities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Merging consecutive NER predictions...\n"
     ]
    }
   ],
   "source": [
    "merge_consecutive_predictions(ner_predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Adjust the indices of predicted entities to reflect the ground truth format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def adjust_predicted_indices(data):\n",
    "    \"\"\"\n",
    "    Adjust the indices of predicted entities in the abstract by subtracting the length of the title\n",
    "    from both the start and end indices, and decreasing the end index by 1.\n",
    "    \"\"\"\n",
    "    print(\"Adjusting indices for NER predictions...\")\n",
    "    # Process each document\n",
    "    for pmid, doc in data.items():\n",
    "        title_length = len(doc.get(\"title\", \"\"))  # Calculate the length of the title\n",
    "\n",
    "        for entity in doc.get(\"pred_entities\", []):\n",
    "            entity[\"end_idx\"] -= 1  # Adjust the end index to be exclusive\n",
    "\n",
    "            if entity[\"tag\"] == \"a\":  # Process only entities from the abstract\n",
    "                entity[\"start_idx\"] -= title_length + 1\n",
    "                entity[\"end_idx\"] -= title_length + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adjusting indices for NER predictions...\n"
     ]
    }
   ],
   "source": [
    "adjust_predicted_indices(ner_predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Convert predicted entities to ground truth format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def migrate_to_ground_truth_format(articles):\n",
    "    return_dict = {}\n",
    "\n",
    "    for pmid, article in articles.items():\n",
    "        return_dict[pmid] = {}\n",
    "        return_dict[pmid]['metadata'] = {}\n",
    "        return_dict[pmid]['entities'] = []\n",
    "        return_dict[pmid]['relations'] = []\n",
    "        \n",
    "        return_dict[pmid]['metadata']['title'] = article['title']\n",
    "        return_dict[pmid]['metadata']['author'] = article['author']\n",
    "        return_dict[pmid]['metadata']['journal'] = article['journal']\n",
    "        return_dict[pmid]['metadata']['year'] = article['year']\n",
    "        return_dict[pmid]['metadata']['abstract'] = article['abstract']\n",
    "        return_dict[pmid]['metadata']['annotator'] = 'distant'\n",
    "\n",
    "        for entity in article['pred_entities']:\n",
    "            ent_dict = {\n",
    "                \"start_idx\": entity['start_idx'],\n",
    "                \"end_idx\": entity['end_idx'],\n",
    "                \"location\": 'title' if entity['tag'] == 't' else 'abstract',\n",
    "                \"text_span\": entity['text_span'],\n",
    "                \"label\": entity['entity_label']\n",
    "            }\n",
    "            return_dict[pmid]['entities'].append(ent_dict)\n",
    "\n",
    "    return return_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = migrate_to_ground_truth_format(ner_predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(PATH_OUTPUT_NER_PREDICTIONS, 'w', encoding='utf-8') as file:\n",
    "    json.dump(predictions, file, indent=2)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gutbrainie",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
