{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convert annotations to the format used by GLiNER for finetuning."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define the imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define paths to the annotation files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH_PLATINUM_TRAIN = \"../Annotations/Train/platinum_quality/json_format/train_platinum.json\"\n",
    "PATH_GOLD_TRAIN = \"../Annotations/Train/gold_quality/json_format/train_gold.json\"\n",
    "PATH_SILVER_TRAIN = \"../Annotations/Train/silver_quality/json_format/train_silver.json\"\n",
    "PATH_BRONZE_TRAIN = \"../Annotations/Train/bronze_quality/json_format/train_bronze.json\"\n",
    "PATH_DEV = \"../Annotations/Dev/json_format/dev.json\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define output paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH_OUTPUT_PLATINUM_TRAIN = \"../Train/NER/data/train_platinum.json\"\n",
    "PATH_OUTPUT_GOLD_TRAIN = \"../Train/NER/data/train_gold.json\"\n",
    "PATH_OUTPUT_SILVER_TRAIN = \"../Train/NER/data/train_silver.json\"\n",
    "PATH_OUTPUT_BRONZE_TRAIN = \"../Train/NER/data/train_bronze.json\"\n",
    "PATH_OUTPUT_DEV = \"../Train/NER/data/dev.json\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the input files into dictionary variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(PATH_PLATINUM_TRAIN, 'r', encoding='utf-8') as file:\n",
    "\ttrain_platinum = json.load(file)\n",
    "\n",
    "with open(PATH_GOLD_TRAIN, 'r', encoding='utf-8') as file:\n",
    "\ttrain_gold = json.load(file)\n",
    "\n",
    "with open(PATH_SILVER_TRAIN, 'r', encoding='utf-8') as file:\n",
    "\ttrain_silver = json.load(file)\n",
    "\t\n",
    "with open(PATH_BRONZE_TRAIN, 'r', encoding='utf-8') as file:\n",
    "\ttrain_bronze = json.load(file)\n",
    "\n",
    "with open(PATH_DEV, 'r', encoding='utf-8') as file:\n",
    "\tdev = json.load(file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define the function to parse the annotations to the GLiNER finetuning format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_text_with_positions(text):\n",
    "    # Split text into tokens, preserving punctuation (except for hyphens and underscores)\n",
    "    tokens = []\n",
    "    token_spans = []  # list of (start_char_index, end_char_index) for each token\n",
    "    pattern = re.compile(r\"\\w+|[.,!?;:\\'\\\"()\\[\\]{}<>]|[\\s]+|\\S\")\n",
    "    for match in pattern.finditer(text):\n",
    "        token = match.group()\n",
    "        if token.isspace():\n",
    "            continue  # Skip whitespace tokens\n",
    "        start_pos = match.start()\n",
    "        if re.match(r\"\\w+-\\w+\", token) or re.match(r\"\\w+_\\w+\", token):\n",
    "            # Keep hyphenated or underscored words intact\n",
    "            tokens.append(token)\n",
    "            token_spans.append((start_pos, match.end()))\n",
    "        else:\n",
    "            # Split contractions (e.g., \"don't\" -> \"don\", \"'\", \"t\")\n",
    "            contraction_match = re.match(r\"(\\w+)(')(\\w+)\", token)\n",
    "            if contraction_match:\n",
    "                groups = contraction_match.groups()\n",
    "                for group in groups:\n",
    "                    end_pos = start_pos + len(group)\n",
    "                    tokens.append(group)\n",
    "                    token_spans.append((start_pos, end_pos))\n",
    "                    start_pos = end_pos\n",
    "            else:\n",
    "                tokens.append(token)\n",
    "                token_spans.append((start_pos, match.end()))\n",
    "    return tokens, token_spans\n",
    "\n",
    "def process_annotations(data):\n",
    "    output_data = []\n",
    "\n",
    "    for doc_id, doc_data in data.items():\n",
    "        overall_tokenized_text = []\n",
    "        overall_ner = []\n",
    "        token_offset = 0\n",
    "\n",
    "        fields = [\"title\", \"abstract\"]\n",
    "\n",
    "        for field in fields:\n",
    "            text = doc_data[\"metadata\"].get(field, \"\")\n",
    "            tokens, token_spans = tokenize_text_with_positions(text)\n",
    "\n",
    "            # Collect entities for this field\n",
    "            field_entities = []\n",
    "            for entity in doc_data.get(\"entities\", []):\n",
    "                mention_location = entity.get(\"location\", \"\")\n",
    "                if mention_location == field:\n",
    "                    field_entities.append(entity)\n",
    "\n",
    "            # Map entities from character indices to token indices\n",
    "            for entity in field_entities:\n",
    "                entity_start_char = entity[\"start_idx\"]\n",
    "                entity_end_char = entity[\"end_idx\"] + 1  # Adjusting end index to be exclusive\n",
    "                entity_label = entity[\"label\"]\n",
    "\n",
    "                entity_start_token_index = None\n",
    "                entity_end_token_index = None\n",
    "\n",
    "                for i, (token_start_char, token_end_char) in enumerate(token_spans):\n",
    "                    if token_end_char <= entity_start_char:\n",
    "                        continue  # Token is before the entity\n",
    "                    if token_start_char >= entity_end_char:\n",
    "                        break  # Token is after the entity\n",
    "                    # Token overlaps with entity\n",
    "                    if entity_start_token_index is None:\n",
    "                        entity_start_token_index = i\n",
    "                    entity_end_token_index = i  # Update to the last overlapping token\n",
    "\n",
    "                if entity_start_token_index is not None and entity_end_token_index is not None:\n",
    "                    overall_ner.append([\n",
    "                        entity_start_token_index + token_offset,\n",
    "                        entity_end_token_index + token_offset,\n",
    "                        entity_label.lower()\n",
    "                    ])\n",
    "                else:\n",
    "                    print(f\"Warning: Could not find tokens for entity in doc {doc_id}, field {field}\")\n",
    "\n",
    "            # Append tokens to the overall tokenized text\n",
    "            overall_tokenized_text.extend(tokens)\n",
    "            token_offset += len(tokens)\n",
    "\n",
    "        # Sort the word positions by the start index\n",
    "        overall_ner.sort(key=lambda x: x[0])\n",
    "\n",
    "        # Create the output dictionary for this document\n",
    "        output_doc = {\n",
    "            \"tokenized_text\": overall_tokenized_text,\n",
    "            \"ner\": overall_ner\n",
    "        }\n",
    "\n",
    "        output_data.append(output_doc)\n",
    "\n",
    "    return output_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "processed_train_platinum = process_annotations(train_platinum)\n",
    "processed_train_gold = process_annotations(train_gold)\n",
    "processed_train_silver = process_annotations(train_silver)\n",
    "processed_train_bronze = process_annotations(train_bronze)\n",
    "processed_dev = process_annotations(dev)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dump to json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dump_to_json(dict, output_file_path):\n",
    "\twith open(output_file_path, 'w', encoding='utf-8') as f:\n",
    "\t\t#json.dump(dict, f, indent=2)\n",
    "\t\tjson.dump(dict, f)\n",
    "\n",
    "dump_to_json(processed_train_platinum, PATH_OUTPUT_PLATINUM_TRAIN)\n",
    "dump_to_json(processed_train_gold, PATH_OUTPUT_GOLD_TRAIN)\n",
    "dump_to_json(processed_train_silver, PATH_OUTPUT_SILVER_TRAIN)\n",
    "dump_to_json(processed_train_bronze, PATH_OUTPUT_BRONZE_TRAIN)\n",
    "dump_to_json(processed_dev, PATH_OUTPUT_DEV)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "bioasq-2025-task6_2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
