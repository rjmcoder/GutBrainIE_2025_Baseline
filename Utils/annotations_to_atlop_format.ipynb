{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convert annotations to the format used by ATLOP for finetuning."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define the imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "print('Nltk version: {}.'.format(nltk.__version__))\n",
    "\n",
    "from nltk.tokenize import TreebankWordTokenizer as twt\n",
    "from nltk.tokenize import WordPunctTokenizer as wpt\n",
    "from nltk.tokenize.punkt import PunktSentenceTokenizer, PunktParameters\n",
    "nltk.download('punkt_tab')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define paths to the annotation files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH_PLATINUM_TRAIN = \"../Annotations/Train/platinum_quality/json_format/train_platinum.json\"\n",
    "PATH_GOLD_TRAIN = \"../Annotations/Train/gold_quality/json_format/train_gold.json\"\n",
    "PATH_SILVER_TRAIN = \"../Annotations/Train/silver_quality/json_format/train_silver.json\"\n",
    "PATH_BRONZE_TRAIN = \"../Annotations/Train/bronze_quality/json_format/train_bronze.json\"\n",
    "PATH_DEV = \"../Annotations/Dev/json_format/dev.json\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define output paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH_OUTPUT_PLATINUM_TRAIN = \"../Train/RE/data/train_platinum.json\"\n",
    "PATH_OUTPUT_GOLD_TRAIN = \"../Train/RE/data/train_gold.json\"\n",
    "PATH_OUTPUT_SILVER_TRAIN = \"../Train/RE/data/train_silver.json\"\n",
    "PATH_OUTPUT_BRONZE_TRAIN = \"../Train/RE/data/train_bronze.json\"\n",
    "PATH_OUTPUT_DEV = \"../Train/RE/data/dev.json\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the input files into dictionary variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(PATH_PLATINUM_TRAIN, 'r', encoding='utf-8') as file:\n",
    "\ttrain_platinum = json.load(file)\n",
    "\n",
    "with open(PATH_GOLD_TRAIN, 'r', encoding='utf-8') as file:\n",
    "\ttrain_gold = json.load(file)\n",
    "\n",
    "with open(PATH_SILVER_TRAIN, 'r', encoding='utf-8') as file:\n",
    "\ttrain_silver = json.load(file)\n",
    "\t\n",
    "with open(PATH_BRONZE_TRAIN, 'r', encoding='utf-8') as file:\n",
    "\ttrain_bronze = json.load(file)\n",
    "\n",
    "with open(PATH_DEV, 'r', encoding='utf-8') as file:\n",
    "\tdev = json.load(file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remove articles having less that 5 entities annotated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_articles_with_less_than_5_entities(data: dict):\n",
    "    pmid_list = []\n",
    "    for pmid, article in data.items():\n",
    "        entities = article['entities']\n",
    "        if len(entities) < 5:\n",
    "            pmid_list.append(pmid)\n",
    "    return pmid_list\n",
    "\n",
    "def remove_articles_with_less_than_5_entities(data: dict, data_name: str):\n",
    "    pmid_list = get_articles_with_less_than_5_entities(data)\n",
    "    for pmid in pmid_list:\n",
    "        del data[pmid]\n",
    "    print(f'{data_name} - {len(pmid_list)} articles removed.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "remove_articles_with_less_than_5_entities(train_platinum, \"train_platinum\")\n",
    "remove_articles_with_less_than_5_entities(train_gold, \"train_gold\")\n",
    "remove_articles_with_less_than_5_entities(train_silver, \"train_silver\")\n",
    "remove_articles_with_less_than_5_entities(train_bronze, \"train_bronze\")\n",
    "remove_articles_with_less_than_5_entities(dev, \"dev\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remove articles without relations annotated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_articles_without_relations(data: dict, data_name: str):\n",
    "    pmid_list = []\n",
    "    for pmid, article in data.items():\n",
    "        if len(article['relations']) == 0:\n",
    "            pmid_list.append(pmid)\n",
    "\n",
    "    for pmid in pmid_list:\n",
    "        del data[pmid]\n",
    "    print(f'{data_name} - {len(pmid_list)} articles removed.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "remove_articles_without_relations(train_platinum, \"train_platinum\")\n",
    "remove_articles_without_relations(train_gold, \"train_gold\")\n",
    "remove_articles_without_relations(train_silver, \"train_silver\")\n",
    "remove_articles_without_relations(train_bronze, \"train_bronze\")\n",
    "remove_articles_without_relations(dev, \"dev\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tokenize the articles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the tokens of length 2 that are not captured by the tokenizer\n",
    "ILLEGAL_WORDS_2 = [').', '(<', '>)', '),', '.,', '].', '],', '.:', '>.', '>,', '))', '+)', '>-', '</', '[<', '-,', '.)', 'â„¢,']\n",
    "# Define the tokens of length 3 that are not captured by the tokenizer\n",
    "ILLEGAL_WORDS_3 = ['.),', '.].', '>),', '.).', '>).']\n",
    "\n",
    "def tokenize_docs(data: dict, data_name: str):\n",
    "\tprint(f\"Tokenizing articles in set {data_name}...\")\n",
    "\n",
    "\tfor pmid, article in data.items():\n",
    "\t\ttitle = article['metadata']['title']\n",
    "\t\tabstract = article['metadata']['abstract']\n",
    "\n",
    "\t\ttitle_spans = list(wpt().span_tokenize(title))\n",
    "\t\tabstract_spans = list(wpt().span_tokenize(abstract))\n",
    "\t\t\n",
    "\t\tarticle['tokenized_title'] = []\n",
    "\t\tfor start, end in title_spans:\n",
    "\t\t\tword = title[start:end]\n",
    "\t\t\tif word in ILLEGAL_WORDS_2:\n",
    "\t\t\t\tword1 = title[start:end-1]\n",
    "\t\t\t\tword2 = title[end-1:end]\n",
    "\t\t\t\tarticle['tokenized_title'].append((word1, start, end-1))\n",
    "\t\t\t\tarticle['tokenized_title'].append((word2, end-1, end))\n",
    "\t\t\telif word in ILLEGAL_WORDS_3:\n",
    "\t\t\t\tword1 = title[start:start+1]\n",
    "\t\t\t\tword2 = title[start+1:end-1]\n",
    "\t\t\t\tword3 = title[end-1:end]\n",
    "\t\t\t\tarticle['tokenized_title'].append((word1, start, start+1))\n",
    "\t\t\t\tarticle['tokenized_title'].append((word2, start+1, end-1))\n",
    "\t\t\t\tarticle['tokenized_title'].append((word3, end-1, end))\n",
    "\t\t\telse:\t\n",
    "\t\t\t\tarticle['tokenized_title'].append((word, start, end))\n",
    "\t\t\n",
    "\t\tarticle['tokenized_abstract'] = []\n",
    "\t\tfor start, end in abstract_spans:\n",
    "\t\t\tword = abstract[start:end]\n",
    "\t\t\tif word in ILLEGAL_WORDS_2:\n",
    "\t\t\t\tword1 = abstract[start:end-1]\n",
    "\t\t\t\tword2 = abstract[end-1:end]\n",
    "\t\t\t\tarticle['tokenized_abstract'].append((word1, start, end-1))\n",
    "\t\t\t\tarticle['tokenized_abstract'].append((word2, end-1, end))\n",
    "\t\t\telif word in ILLEGAL_WORDS_3:\n",
    "\t\t\t\tword1 = abstract[start:start+1]\n",
    "\t\t\t\tword2 = abstract[start+1:end-1]\n",
    "\t\t\t\tword3 = abstract[end-1:end]\n",
    "\t\t\t\tarticle['tokenized_abstract'].append((word1, start, start+1))\n",
    "\t\t\t\tarticle['tokenized_abstract'].append((word2, start+1, end-1))\n",
    "\t\t\t\tarticle['tokenized_abstract'].append((word3, end-1, end))\n",
    "\t\t\telse:\n",
    "\t\t\t\tarticle['tokenized_abstract'].append((word, start, end))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenize_docs(train_platinum, \"train_platinum\")\n",
    "tokenize_docs(train_gold, \"train_gold\")\n",
    "tokenize_docs(train_silver, \"train_silver\")\n",
    "tokenize_docs(train_bronze, \"train_bronze\")\n",
    "tokenize_docs(dev, \"dev\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Adjust wrong annotations (i.e., annotations including partially annotated words) from the silver collection "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Each PMIDs maps to a dict of {annotation_with_wrong_text_span: correct_text_span}\n",
    "PARTIAL_WORDS = {\n",
    "    '35275534': {\n",
    "        (1395, 1412, 'abstract', 'Ruminococcusgnavus'): 'Ruminococcusgnavusgroup'  \n",
    "    },\n",
    "    '38963982': {\n",
    "        (92, 94, 'title', 'TAM'): 'TAMs',\n",
    "        (435, 451, 'abstract', 'Intestinal tissue'): 'Intestinal tissues'\n",
    "    },\n",
    "    '38959280': {\n",
    "        (74, 76, 'abstract', 'SGM'): 'SGMs',\n",
    "        (695, 697, 'abstract', 'SGM'): 'SGMs',\n",
    "        (266, 287, 'abstract', 'cisgender heterosexual'): 'cisgender heterosexuals',\n",
    "        (713, 734, 'abstract', 'cisgender-heterosexual'): 'cisgender-heterosexuals' \n",
    "    },\n",
    "    '38968876': {\n",
    "        (764, 770, 'abstract', 'patient'): 'patients'\n",
    "    },\n",
    "    '38892525': {\n",
    "        (1397, 1407, 'abstract', 'IBS symptom'): 'IBS symptoms'\n",
    "    }\n",
    "}\n",
    "\n",
    "def fix_wrong_annotations(data: dict, data_name: str):\n",
    "    print(f'Fixing annotations in set {data_name}...')\n",
    "    \n",
    "    for pmid in list(data.keys()):\n",
    "        pmid_str = str(pmid)\n",
    "        if pmid_str in PARTIAL_WORDS:\n",
    "            replacements = PARTIAL_WORDS[pmid_str]\n",
    "            \n",
    "            # Fix entities:\n",
    "            for entity in data[pmid]['entities']:\n",
    "                wrong_entry = (entity['start_idx'], entity['end_idx'], entity['location'], entity['text_span'])\n",
    "                if wrong_entry in replacements:\n",
    "                    correct_text = replacements[wrong_entry]\n",
    "                    entity['text_span'] = correct_text\n",
    "                    # Update end index based on the new text length.\n",
    "                    entity['end_idx'] = entity['start_idx'] + len(correct_text) - 1\n",
    "                    print(f\"Fixed entity in pmid {pmid}: '{wrong_entry[3]}' -> '{correct_text}'\")\n",
    "            \n",
    "            # Fix relations:\n",
    "            for relation in data[pmid]['relations']:\n",
    "                # Check and fix subject if needed.\n",
    "                wrong_entry = (relation['subject_start_idx'], relation['subject_end_idx'], relation['subject_location'], relation['subject_text_span'])\n",
    "                if wrong_entry in replacements:\n",
    "                    correct_text = replacements[wrong_entry]\n",
    "                    relation['subject_text_span'] = correct_text\n",
    "                    relation['subject_end_idx'] = relation['subject_start_idx'] + len(correct_text) - 1\n",
    "                    print(f\"Fixed subject in pmid {pmid}: '{wrong_entry[3]}' -> '{correct_text}'\")\n",
    "                    \n",
    "                # Check and fix object if needed.\n",
    "                wrong_entry = (relation['object_start_idx'], relation['object_end_idx'], relation['object_location'], relation['object_text_span'])\n",
    "                if wrong_entry in replacements:\n",
    "                    correct_text = replacements[wrong_entry]\n",
    "                    relation['object_text_span'] = correct_text\n",
    "                    relation['object_end_idx'] = relation['object_start_idx'] + len(correct_text) - 1\n",
    "                    print(f\"Fixed object in pmid {pmid}: '{wrong_entry[3]}' -> '{correct_text}'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fix_wrong_annotations(train_silver, 'train_silver')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Map annotated entities to tokens "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def map_entities_to_tokens(data: dict, data_name: str):\n",
    "\tprint(f\"Mapping entities to tokens in set {data_name}...\")\n",
    "\t\n",
    "\tfor pmid, article in data.items():\n",
    "\t\tfor entity in article['entities']:\n",
    "\t\t\tlocation = entity['location']\n",
    "\t\t\tstart = entity['start_idx']\n",
    "\t\t\tend = entity['end_idx']\n",
    "\t\t\tstart_token = None\n",
    "\t\t\tend_token = None\n",
    "\t\t\tif location == 'title':\n",
    "\t\t\t\tfor idx, token in enumerate(article['tokenized_title']):\n",
    "\t\t\t\t\tif start == token[1] and start is not None:\n",
    "\t\t\t\t\t\tstart_token = idx\n",
    "\t\t\t\t\tif end == token[2]-1 and end is not None:\n",
    "\t\t\t\t\t\tend_token = idx\n",
    "\t\t\telif location == 'abstract':\n",
    "\t\t\t\tfor idx, token in enumerate(article['tokenized_abstract']):\n",
    "\t\t\t\t\tif start == token[1] and start is not None:\n",
    "\t\t\t\t\t\tstart_token = idx\n",
    "\t\t\t\t\tif end == token[2]-1 and end is not None:\n",
    "\t\t\t\t\t\tend_token = idx\n",
    "\t\t\telse:\n",
    "\t\t\t\traise Exception(f'{pmid} - Unrecognized Location: {location}')\n",
    "\t\t\tif start_token is not None and end_token is not None:\n",
    "\t\t\t\tentity['start_token'] = start_token\n",
    "\t\t\t\tentity['end_token'] = end_token\n",
    "\t\t\telse:\n",
    "\t\t\t\tprint (data[pmid]['tokenized_title'])\n",
    "\t\t\t\tprint(data[pmid]['tokenized_abstract'])\n",
    "\t\t\t\traise Exception(f'{pmid} - Not able to assign token(s) to entity: {entity}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "map_entities_to_tokens(train_platinum, \"train_platinum\")\n",
    "map_entities_to_tokens(train_gold, \"train_gold\")\n",
    "map_entities_to_tokens(train_silver, \"train_silver\")\n",
    "map_entities_to_tokens(train_bronze, \"train_bronze\")\n",
    "map_entities_to_tokens(dev, \"dev\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get the start and end indices for articles sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define abbreviations to not be splitted by the sentence tokenizer\n",
    "extra_abbrevs = {'etc', 'etc.', 'etc.)', '<i>L', 'sp', 'subsp', '<i>A', '(<i>Hippophae rhamnoides</i> L.)'}\n",
    "punkt_param = PunktParameters()\n",
    "for abbr in extra_abbrevs:\n",
    "\tpunkt_param.abbrev_types.add(abbr)\n",
    "sentence_splitter = PunktSentenceTokenizer(punkt_param)\n",
    "\n",
    "def get_sentence_spans(data: dict, data_name: str):\n",
    "    print(f\"Getting sentence spans in set {data_name}...\")\n",
    "    \n",
    "    for pmid, article in data.items():\n",
    "        title = article['metadata']['title']\n",
    "        abstract = article['metadata']['abstract']\n",
    "\n",
    "        # Convert the generator to a list so we can iterate it repeatedly.\n",
    "        sentences = list(sentence_splitter.span_tokenize(abstract))\n",
    "\n",
    "        # Prepare a list of booleans that will flag whether the sentence at a given index should be merged with the next one. \n",
    "        # A sentence is merged with the following one if an entity spans across them.\n",
    "        # Initially, no merge is flagged.\n",
    "        merge_next = [False] * len(sentences)\n",
    "        \n",
    "        # Process each entity in the article.\n",
    "        for entity in article['entities']:\n",
    "            location = entity['location']\n",
    "            start = entity['start_idx']\n",
    "            end = entity['end_idx']\n",
    "\n",
    "            # For title entities, we do nothing regarding sentence spans.\n",
    "            if location == 'title':\n",
    "                if end > len(title):\n",
    "                    raise Exception(f'{pmid} - Found title entity having illegal end index: {entity}')\n",
    "                continue\n",
    "\n",
    "            # Only process abstract entities.\n",
    "            if location == 'abstract':\n",
    "                start_sentence = None\n",
    "                end_sentence = None\n",
    "\n",
    "                # Iterate over the original sentence spans to determine in which sentences the entity start and end fall.\n",
    "                for idx, s in enumerate(sentences):\n",
    "                    # Using >= and <= to include boundaries.\n",
    "                    if start >= s[0] and start <= s[1] and start_sentence is None:\n",
    "                        start_sentence = idx\n",
    "                        #print(f'Start sentence assigned: {idx}')\n",
    "                    if end >= s[0] and end <= s[1] and end_sentence is None:\n",
    "                        end_sentence = idx\n",
    "                        #print(f'End sentence assigned: {idx}')\n",
    "\n",
    "                if start_sentence is None:\n",
    "                    raise Exception(f'{pmid} - Start sentence not assigned for entity: {entity}')\n",
    "                if end_sentence is None:\n",
    "                    raise Exception(f'{pmid} - End sentence not assigned for entity: {entity}')\n",
    "                \n",
    "                # If the entity falls in two different sentences, check if they are consecutive.\n",
    "                if start_sentence != end_sentence:\n",
    "                    if end_sentence - start_sentence == 1:\n",
    "                        # Mark that sentence 'start_sentence' should be merged with its following sentence.\n",
    "                        merge_next[start_sentence] = True\n",
    "                        #print(f'{pmid} - Marking merge for sentences {start_sentence} and {end_sentence} due to entity: {entity}')\n",
    "                    else:\n",
    "                        raise Exception(f'{pmid} - Entity assigned to two non-consecutive sentences ({start_sentence}, {end_sentence}): {entity}')\n",
    "        \n",
    "        # At this point, we have a merge flag for each sentence that should be merged with its next one.\n",
    "        # Now we build the updated list of sentence spans, merging as flagged.\n",
    "        new_spans = []\n",
    "        i = 0\n",
    "        while i < len(sentences):\n",
    "            start_val = sentences[i][0]\n",
    "            end_val = sentences[i][1]\n",
    "            # While the current sentence is flagged to merge with the next one, update the end_val.\n",
    "            while i < len(sentences) - 1 and merge_next[i]:\n",
    "                i += 1\n",
    "                end_val = sentences[i][1]\n",
    "            new_spans.append((start_val, end_val))\n",
    "            i += 1\n",
    "\n",
    "        # Add the updated list of sentence spans to the article dictionary.\n",
    "        article['sentences'] = new_spans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_sentence_spans(train_platinum, \"train_platinum\")\n",
    "get_sentence_spans(train_gold, \"train_gold\")\n",
    "get_sentence_spans(train_silver, \"train_silver\")\n",
    "get_sentence_spans(train_bronze, \"train_bronze\")\n",
    "get_sentence_spans(dev, \"dev\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check if sentence spans have been computed correctly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_sentence_spans(data: dict, data_name: str):\n",
    "    print(f\"Checking sentence spans in set {data_name}...\")\n",
    "\n",
    "    for pmid, article in data.items():\n",
    "        # Process each entity in the article.\n",
    "        for entity in article['entities']:\n",
    "            location = entity['location']\n",
    "            start = entity['start_idx']\n",
    "            end = entity['end_idx']\n",
    "            # For title entities, we do nothing regarding sentence spans.\n",
    "            if location == 'title':\n",
    "                continue\n",
    "\n",
    "            # Only process abstract entities.\n",
    "            if location == 'abstract':\n",
    "                start_sentence = None\n",
    "                end_sentence = None\n",
    "\n",
    "                # Iterate over the original sentence spans to determine in which sentences the entity start and end fall.\n",
    "                for idx, s in enumerate(article['sentences']):\n",
    "                    # Using >= and <= to include boundaries.\n",
    "                    if start >= s[0] and start <= s[1] and start_sentence is None:\n",
    "                        start_sentence = idx\n",
    "                        #print(f'Start sentence assigned: {idx}')\n",
    "                    if end >= s[0] and end <= s[1] and end_sentence is None:\n",
    "                        end_sentence = idx\n",
    "                        #print(f'End sentence assigned: {idx}')\n",
    "\n",
    "                if start_sentence is None:\n",
    "                    raise Exception(f'{pmid} - Start sentence not assigned for entity: {entity}')\n",
    "                if end_sentence is None:\n",
    "                    raise Exception(f'{pmid} - End sentence not assigned for entity: {entity}')\n",
    "                \n",
    "                # If the entity falls in two different sentences, raise Exception.\n",
    "                if start_sentence != end_sentence:\n",
    "                      raise Exception(f'{pmid} - Entity assigned to two different sentences ({start_sentence}, {end_sentence}): {entity}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "check_sentence_spans(train_platinum, \"train_platinum\")\n",
    "check_sentence_spans(train_gold, \"train_gold\")\n",
    "check_sentence_spans(train_silver, \"train_silver\")\n",
    "check_sentence_spans(train_bronze, \"train_bronze\")\n",
    "check_sentence_spans(dev, \"dev\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Map tokens to the sentence in which they are located"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def map_tokens_to_sentences(data: dict, data_name: str):\n",
    "    \"\"\"\n",
    "    For each article, map tokens in the 'tokenized abstract' to the sentence in which they are located.\n",
    "    Uses the 'sentences' field in the article, which is assumed to be a list of (start, end) tuples.\n",
    "    \n",
    "    The mapping is stored as a dictionary where the key is the token index (its position in the tokenized abstract)\n",
    "    and the value is the sentence index. For example, if the first token belongs to sentence 0 and the third token\n",
    "    belongs to sentence 1, the mapping will include entries {0: 0, 2: 1}.\n",
    "    \n",
    "    Raises an Exception if a token does not fall within any of the sentence spans.\n",
    "    \"\"\"\n",
    "    print(f\"Mapping tokens to sentences in set {data_name}...\")\n",
    "\n",
    "    for pmid, article in data.items():\n",
    "        # Retrieve the tokenized abstract and the sentence spans.\n",
    "        tokens = article.get('tokenized_abstract')\n",
    "        sentences = article.get('sentences')\n",
    "        \n",
    "        if tokens is None:\n",
    "            raise Exception(f\"Article {pmid} is missing 'tokenized abstract'.\")\n",
    "        if sentences is None:\n",
    "            raise Exception(f\"Article {pmid} is missing 'sentences'. Make sure to run get_sentence_spans first.\")\n",
    "        \n",
    "        token_to_sentence = {}\n",
    "        \n",
    "        # Iterate over each token and determine which sentence it belongs to.\n",
    "        for token_index, token_entry in enumerate(tokens):\n",
    "            # Each token_entry is assumed to be a tuple: (token_text, start_offset, end_offset)\n",
    "            token_text, token_start, token_end = token_entry\n",
    "            assigned_sentence = None\n",
    "            \n",
    "            # Check each sentence span to see if the token falls within it.\n",
    "            for sentence_index, (sent_start, sent_end) in enumerate(sentences):\n",
    "                # We assume a token belongs to a sentence if its start is >= sentence start and its end is <= sentence end.\n",
    "                if token_start >= sent_start and token_end <= sent_end:\n",
    "                    assigned_sentence = sentence_index\n",
    "                    break  # Stop once we find the sentence that contains the token.\n",
    "            \n",
    "            if assigned_sentence is None:\n",
    "                raise Exception(\n",
    "                    f\"Token '{token_text}' (index {token_index}, offsets {token_start}-{token_end}) \"\n",
    "                    f\"in article {pmid} does not fall within any sentence span: {sentences}\"\n",
    "                )\n",
    "            \n",
    "            token_to_sentence[token_index] = assigned_sentence\n",
    "        \n",
    "        # Add the mapping to the article dictionary.\n",
    "        article['tokens_to_sentences_map'] = token_to_sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "map_tokens_to_sentences(train_platinum, \"train_platinum\")\n",
    "map_tokens_to_sentences(train_gold, \"train_gold\")\n",
    "map_tokens_to_sentences(train_silver, \"train_silver\")\n",
    "map_tokens_to_sentences(train_bronze, \"train_bronze\")\n",
    "map_tokens_to_sentences(dev, \"dev\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Map entities to the token positions within each sentence containing them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def map_entities_to_tokens_within_sentences(data: dict, data_name: str) -> dict:\n",
    "    \"\"\"\n",
    "    For each article, this function maps each entity (assumed to be in the abstract)\n",
    "    to the token positions within the sentence that contains it.\n",
    "    \n",
    "    For each entity in article['entities'] (with location 'abstract'), it adds:\n",
    "      - 'located_in_sentence': the sentence index in which the entity's tokens are located,\n",
    "      - 'start_token_in_sentence': the position of the entity's start token within that sentence,\n",
    "      - 'end_token_in_sentence': the position of the entity's end token within that sentence.\n",
    "    \n",
    "    This function relies on:\n",
    "      - article['tokenized abstract']: a list of tokens of the form (token_text, start_offset, end_offset)\n",
    "      - article['tokens_to_sentences_map']: a mapping { token_index -> sentence_index }\n",
    "      - article['sentences']: a list of (start, end) sentence spans for the abstract.\n",
    "    \"\"\"\n",
    "    print(f\"Mapping entities to tokens within sentences in set {data_name}...\")\n",
    "    \n",
    "    for pmid, article in data.items():\n",
    "        # Retrieve required fields.\n",
    "        tokens = article.get('tokenized_abstract')\n",
    "        token_to_sentence = article.get('tokens_to_sentences_map')\n",
    "        sentences = article.get('sentences')\n",
    "        \n",
    "        if tokens is None:\n",
    "            raise Exception(f\"Article {pmid} is missing 'tokenized abstract'.\")\n",
    "        if token_to_sentence is None:\n",
    "            raise Exception(f\"Article {pmid} is missing 'tokens_to_sentences_map'. Run map_tokens_to_sentences first.\")\n",
    "        if sentences is None:\n",
    "            raise Exception(f\"Article {pmid} is missing 'sentences'. Run get_sentence_spans first.\")\n",
    "        \n",
    "        # Build a helper mapping: for each sentence index, list the token indices that fall into that sentence.\n",
    "        sentence_to_token_indices = {}\n",
    "        for token_index in range(len(tokens)):\n",
    "            sent_idx = token_to_sentence.get(token_index)\n",
    "            if sent_idx is None:\n",
    "                raise Exception(\n",
    "                    f\"In article {pmid}, token index {token_index} is not mapped to any sentence. Tokens: {tokens[token_index]}\"\n",
    "                )\n",
    "            sentence_to_token_indices.setdefault(sent_idx, []).append(token_index)\n",
    "        \n",
    "        # Now process each entity.\n",
    "        for entity in article.get('entities', []):\n",
    "            if entity.get('location') != 'abstract': # Only process entities in the abstract.\n",
    "                continue\n",
    "            \n",
    "            # Retrieve the token indices for this entity.\n",
    "            entity_start_token = entity.get('start_token')\n",
    "            entity_end_token = entity.get('end_token')\n",
    "            \n",
    "            if entity_start_token is None or entity_end_token is None:\n",
    "                raise Exception(\n",
    "                    f\"Entity in article {pmid} is missing start_token or end_token: {entity}\"\n",
    "                )\n",
    "            \n",
    "            # Determine the sentence in which the entity's tokens are located.\n",
    "            sentence_for_start = token_to_sentence.get(entity_start_token)\n",
    "            sentence_for_end = token_to_sentence.get(entity_end_token)\n",
    "            \n",
    "            if sentence_for_start is None or sentence_for_end is None:\n",
    "                raise Exception(\n",
    "                    f\"Entity in article {pmid} has tokens not mapped to any sentence: {entity}\"\n",
    "                )\n",
    "            \n",
    "            if sentence_for_start != sentence_for_end:\n",
    "                raise Exception(\n",
    "                    f\"Entity in article {pmid} spans multiple sentences (start in {sentence_for_start}, end in {sentence_for_end}): {entity}\"\n",
    "                )\n",
    "            \n",
    "            located_sentence = sentence_for_start  # or sentence_for_end, both are same.\n",
    "            \n",
    "            # Get the list of token indices for the sentence.\n",
    "            tokens_in_sentence = sentence_to_token_indices.get(located_sentence)\n",
    "            if tokens_in_sentence is None:\n",
    "                raise Exception(\n",
    "                    f\"Sentence {located_sentence} not found in helper mapping for article {pmid}.\"\n",
    "                )\n",
    "            \n",
    "            # Find the position within the sentence for the start token.\n",
    "            try:\n",
    "                start_token_in_sentence = tokens_in_sentence.index(entity_start_token)\n",
    "            except ValueError:\n",
    "                raise Exception(\n",
    "                    f\"Entity start token {entity_start_token} not found in sentence tokens {tokens_in_sentence} for article {pmid}.\"\n",
    "                )\n",
    "            \n",
    "            # And the position within the sentence for the end token.\n",
    "            try:\n",
    "                end_token_in_sentence = tokens_in_sentence.index(entity_end_token)\n",
    "            except ValueError:\n",
    "                raise Exception(\n",
    "                    f\"Entity end token {entity_end_token} not found in sentence tokens {tokens_in_sentence} for article {pmid}.\"\n",
    "                )\n",
    "            \n",
    "            # Add the new fields to the entity.\n",
    "            entity['located_in_sentence'] = located_sentence\n",
    "            entity['start_token_in_sentence'] = start_token_in_sentence\n",
    "            entity['end_token_in_sentence'] = end_token_in_sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "map_entities_to_tokens_within_sentences(train_platinum, \"train_platinum\")\n",
    "map_entities_to_tokens_within_sentences(train_gold, \"train_gold\")\n",
    "map_entities_to_tokens_within_sentences(train_silver, \"train_silver\")\n",
    "map_entities_to_tokens_within_sentences(train_bronze, \"train_bronze\")\n",
    "map_entities_to_tokens_within_sentences(dev, \"dev\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Map each relation's subject and object to an index corresponding to their position in the article's 'entities' list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def map_relations_to_entities(data: dict, data_name: str) -> dict:\n",
    "    \"\"\"\n",
    "    For each article, this function maps each relation's subject and object to an index \n",
    "    corresponding to their position in the article's 'entities' list. It adds two new fields \n",
    "    to each relation:\n",
    "      - 'subject_entity_idx': the index of the subject entity in the article's 'entities' list.\n",
    "      - 'object_entity_idx': the index of the object entity in the article's 'entities' list.\n",
    "      \n",
    "    The matching is performed based on:\n",
    "      - For the subject:\n",
    "            relation['subject_location'] == entity['location']\n",
    "            relation['subject_start_idx'] == entity['start_idx']\n",
    "            relation['subject_end_idx'] == entity['end_idx']\n",
    "      - For the object:\n",
    "            relation['object_location'] == entity['location']\n",
    "            relation['object_start_idx'] == entity['start_idx']\n",
    "            relation['object_end_idx'] == entity['end_idx']\n",
    "            \n",
    "    It is assumed that the article['entities'] list is ordered such that title entities come first (starting at index 0)\n",
    "    and abstract entities follow (with indices continuing from the last title entity index).\n",
    "    \"\"\"\n",
    "    print(f\"Mapping relations to entities in set {data_name}...\")\n",
    "    \n",
    "    for pmid, article in data.items():\n",
    "        entities = article.get('entities')\n",
    "        relations = article.get('relations')\n",
    "        \n",
    "        if entities is None:\n",
    "            raise Exception(f\"Article {pmid} is missing the 'entities' field.\")\n",
    "        if relations is None:\n",
    "            # If there are no relations, there's nothing to map.\n",
    "            continue\n",
    "        \n",
    "        # Process each relation.\n",
    "        for relation in relations:\n",
    "            # Map the subject.\n",
    "            subject_idx_found = None\n",
    "            for idx, entity in enumerate(entities):\n",
    "                if (entity.get('location') == relation.get('subject_location') and\n",
    "                    entity.get('start_idx') == relation.get('subject_start_idx') and\n",
    "                    entity.get('end_idx') == relation.get('subject_end_idx')):\n",
    "                    subject_idx_found = idx\n",
    "                    break\n",
    "                    \n",
    "            if subject_idx_found is None:\n",
    "                raise Exception(\n",
    "                    f\"Subject entity not found for relation in article {pmid}: {relation}\"\n",
    "                )\n",
    "            relation['subject_entity_idx'] = subject_idx_found\n",
    "            \n",
    "            # Map the object.\n",
    "            object_idx_found = None\n",
    "            for idx, entity in enumerate(entities):\n",
    "                if (entity.get('location') == relation.get('object_location') and\n",
    "                    entity.get('start_idx') == relation.get('object_start_idx') and\n",
    "                    entity.get('end_idx') == relation.get('object_end_idx')):\n",
    "                    object_idx_found = idx\n",
    "                    break\n",
    "                    \n",
    "            if object_idx_found is None:\n",
    "                raise Exception(\n",
    "                    f\"Object entity not found for relation in article {pmid}: {relation}\"\n",
    "                )\n",
    "            relation['object_entity_idx'] = object_idx_found"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "map_relations_to_entities(train_platinum, \"train_platinum\")\n",
    "map_relations_to_entities(train_gold, \"train_gold\")\n",
    "map_relations_to_entities(train_silver, \"train_silver\")\n",
    "map_relations_to_entities(train_bronze, \"train_bronze\")\n",
    "map_relations_to_entities(dev, \"dev\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Convert processed annotations to the DocRED format used by ATLOP for finetuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_to_docred_format(data: dict, data_name: str, is_test=False) -> list:\n",
    "    \"\"\"\n",
    "    Converts articles (in our intermediate format) to the DocRED format.\n",
    "    \n",
    "    For each article, a new dictionary is produced with the following keys:\n",
    "      - \"vertexSet\": a list of entity mentions (each entity becomes a list with one mention).\n",
    "          Each mention is a dict with:\n",
    "              \"pos\": [start_token_in_sentence, end_token_in_sentence],\n",
    "              \"type\": entity label,\n",
    "              \"sent_id\": sentence id (0 for title; abstract sentences are numbered starting at 1),\n",
    "              \"name\": the entity text span.\n",
    "      - \"labels\": a list of relations, each a dict with:\n",
    "              \"r\": relation predicate/label,\n",
    "              \"h\": subject_entity_idx,\n",
    "              \"t\": object_entity_idx,\n",
    "              \"evidence\": a list of two sentence ids: [subject sentence id, object sentence id].\n",
    "      - \"title\": the title string of the article.\n",
    "      - \"sents\": a list of lists of tokens. The first entry is the title tokenization and subsequent\n",
    "                 entries are the tokenizations of the abstract sentences.\n",
    "    \n",
    "    For abstract sentence tokenization, we use the sentence spans in article['sentences'] (a list of (start, end) offsets)\n",
    "    and the tokenized abstract (article['tokenized abstract'], where each token is a tuple (token_text, start, end)).\n",
    "    \n",
    "    Returns a list of DocRED-formatted document dictionaries.\n",
    "    \"\"\"\n",
    "    print(f\"Converting articles to DocRED format for set {data_name}...\")\n",
    "\n",
    "    docred_docs = []\n",
    "    for pmid, article in data.items():\n",
    "        # 1. Build the vertexSet.\n",
    "        # Each entity becomes a single mention. We assume that the entities in article['entities'] \n",
    "        # are ordered with title entities first, then abstract entities.\n",
    "        vertexSet = []\n",
    "        for entity in article.get('entities', []):\n",
    "            # Determine the sentence id according to DocRED.\n",
    "            # Title entities are assigned to sentence 0.\n",
    "            if entity.get('location') == 'title':\n",
    "                sent_id = 0\n",
    "            else:\n",
    "                # For abstract entities, we expect a field 'located_in_sentence' computed earlier.\n",
    "                # In our intermediate format abstract sentences are numbered 0,1,... but in DocRED the title is sentence 0.\n",
    "                # So add 1.\n",
    "                sent_id = entity.get('located_in_sentence', 0) + 1\n",
    "\n",
    "            # The token offsets of the entity within its sentence.\n",
    "            if entity['location'] == 'title':\n",
    "                pos = [entity.get('start_token'), entity.get('end_token')+1]\n",
    "            else:\n",
    "                pos = [entity.get('start_token_in_sentence'), entity.get('end_token_in_sentence')+1]\n",
    "            mention = {\n",
    "                \"pos\": pos,\n",
    "                \"type\": entity.get(\"label\").upper(),\n",
    "                \"sent_id\": sent_id,\n",
    "                \"name\": entity.get(\"text_span\")\n",
    "            }\n",
    "            # Each vertexSet entry is a list of mentions (we have one per entity).\n",
    "            vertexSet.append([mention])\n",
    "        \n",
    "        # 2. Build the labels (relations).\n",
    "        labels = []\n",
    "        for relation in article.get(\"relations\", []):\n",
    "            # Get the indices of the subject and object entities.\n",
    "            subj_idx = relation.get(\"subject_entity_idx\")\n",
    "            obj_idx = relation.get(\"object_entity_idx\")\n",
    "            if subj_idx is None or obj_idx is None:\n",
    "                raise Exception(f\"Missing entity mapping in relation: {relation} in article {pmid}\")\n",
    "            subj_entity = article[\"entities\"][subj_idx]\n",
    "            obj_entity = article[\"entities\"][obj_idx]\n",
    "            \n",
    "            # Determine the sentence id for each entity.\n",
    "            if subj_entity.get(\"location\") == \"title\":\n",
    "                subj_sent = 0\n",
    "            else:\n",
    "                subj_sent = subj_entity.get(\"located_in_sentence\", 0) + 1\n",
    "                \n",
    "            if obj_entity.get(\"location\") == \"title\":\n",
    "                obj_sent = 0\n",
    "            else:\n",
    "                obj_sent = obj_entity.get(\"located_in_sentence\", 0) + 1\n",
    "            \n",
    "            relation_dict = {\n",
    "                \"r\": relation.get(\"predicate\").upper(),\n",
    "                \"h\": subj_idx,\n",
    "                \"t\": obj_idx,\n",
    "                \"evidence\": [subj_sent, obj_sent] if subj_sent != obj_sent else [subj_sent]\n",
    "            }\n",
    "            labels.append(relation_dict)\n",
    "        \n",
    "        # 3. Build the sents field.\n",
    "        # The first sentence is the tokenization of the title.\n",
    "\t\t\n",
    "        title_tokens = article.get(\"tokenized_title\", [])\n",
    "        tokens_in_title = []\n",
    "        for token in title_tokens:\n",
    "            tokens_in_title.append(token[0])\n",
    "        sents = [tokens_in_title]\n",
    "            \n",
    "        # For the abstract sentences, we use article['sentences'] (list of (start, end) spans)\n",
    "        # and article['tokenized abstract'] (list of tokens, each as (token_text, start, end)).\n",
    "        abstract_tokens = article.get(\"tokenized_abstract\", [])\n",
    "        abstract_sents = []\n",
    "        for span in article.get(\"sentences\", []):\n",
    "            s_start, s_end = span\n",
    "            tokens_in_sentence = []\n",
    "            for token, t_start, t_end in abstract_tokens:\n",
    "                # If a token falls completely within the sentence span, add it.\n",
    "                if t_start >= s_start and t_end <= s_end:\n",
    "                    tokens_in_sentence.append(token)\n",
    "            abstract_sents.append(tokens_in_sentence)\n",
    "        sents.extend(abstract_sents)\n",
    "        \n",
    "        # 4. The title string.\n",
    "        doc_title = article[\"metadata\"][\"title\"]\n",
    "        \n",
    "        # 5. Build the final document dictionary.\n",
    "        if is_test:\n",
    "            doc = {\n",
    "                \"vertexSet\": vertexSet,\n",
    "                \"title\": doc_title,\n",
    "                \"sents\": sents\n",
    "            }\n",
    "        else:\n",
    "            doc = {\n",
    "                \"vertexSet\": vertexSet,\n",
    "                \"labels\": labels,\n",
    "                \"title\": doc_title,\n",
    "                \"sents\": sents\n",
    "            }\n",
    "        docred_docs.append(doc)\n",
    "    \n",
    "    return docred_docs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "docred_train_platinum = convert_to_docred_format(train_platinum, \"train_platinum\")\n",
    "docred_train_gold = convert_to_docred_format(train_gold, \"train_gold\")\n",
    "docred_train_silver = convert_to_docred_format(train_silver, \"train_silver\")\n",
    "docred_train_bronze = convert_to_docred_format(train_bronze, \"train_bronze\")\n",
    "docred_dev = convert_to_docred_format(dev, \"dev\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dump the dictionary variable to a json file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dump_to_json(docred_dict, output_file_path):\n",
    "\tdict_with_double_quotes = json.dumps(docred_dict, ensure_ascii=False)\n",
    "\twith open(output_file_path, 'w', encoding='utf-8') as f:\n",
    "\t\tf.write(dict_with_double_quotes)\n",
    "\n",
    "dump_to_json(docred_train_platinum, PATH_OUTPUT_PLATINUM_TRAIN)\n",
    "dump_to_json(docred_train_gold, PATH_OUTPUT_GOLD_TRAIN)\n",
    "dump_to_json(docred_train_silver, PATH_OUTPUT_SILVER_TRAIN)\n",
    "dump_to_json(docred_train_bronze, PATH_OUTPUT_BRONZE_TRAIN)\n",
    "dump_to_json(docred_dev, PATH_OUTPUT_DEV)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
